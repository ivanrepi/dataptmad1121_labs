{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import lxml"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "html_response = requests.get(url).content\n",
    "soup = BeautifulSoup(html_response, \"html.parser\") "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "trending_developers = [element.a.text for element in soup.find_all(\"h1\", {\"class\": \"h3 lh-condensed\"})]\n",
    "trending_developers_list=[dev.replace(\"\\n\",\"\").strip() for dev in trending_developers]\n",
    "trending_developers_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Jared Palmer',\n",
       " 'Geoff Bourne',\n",
       " 'Mathias Fußenegger',\n",
       " 'Florian Roth',\n",
       " 'Guillaume Gomez',\n",
       " 'Henrik Rydgård',\n",
       " 'Jan De Dobbeleer',\n",
       " 'Yo-An Lin',\n",
       " 'Marten Seemann',\n",
       " 'Xaymar',\n",
       " 'David Tolnay',\n",
       " 'snipe',\n",
       " 'XhmikosR',\n",
       " 'Yann Collet',\n",
       " 'bdring',\n",
       " 'Samuel Colvin',\n",
       " 'Alex Gaynor',\n",
       " 'Nico Schlömer',\n",
       " 'Yair Morgenstern',\n",
       " 'Federico Brigante',\n",
       " 'Barry vd. Heuvel',\n",
       " 'MichaIng',\n",
       " 'Dan Davison',\n",
       " 'David Peter',\n",
       " 'Jelle Zijlstra']"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_2 = 'https://github.com/trending/python?since=daily'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "html_response_2 = requests.get(url_2).content\n",
    "soup_2 = BeautifulSoup(html_response_2, \"html.parser\") \n",
    "trending_repos = [element.a.text for element in soup_2.find_all(\"h1\", {\"class\": \"h3 lh-condensed\"})]\n",
    "trending_repos_list=[repo.replace(\"\\n\",\"\").replace(\" \",\"\").replace(\"/\",\" / \") for repo in trending_repos]\n",
    "trending_repos_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Neo23x0 / log4shell-detector',\n",
       " 'TencentARC / GFPGAN',\n",
       " 'WazeHell / sam-the-admin',\n",
       " 'blakeblackshear / frigate',\n",
       " 'sxyu / svox2',\n",
       " 'CorentinJ / Real-Time-Voice-Cloning',\n",
       " 'NorthwaveSecurity / log4jcheck',\n",
       " 'google-research / scenic',\n",
       " 'takito1812 / log4j-detect',\n",
       " 'rwightman / pytorch-image-models',\n",
       " 'jina-ai / jina',\n",
       " 'medvm / widevine_keys',\n",
       " 'alexandre-lavoie / python-log4rce',\n",
       " 'DA-southampton / NLP_ability',\n",
       " 'mlflow / mlflow',\n",
       " 'jessevig / bertviz',\n",
       " 'wpeebles / gangealing',\n",
       " 'aeon0 / botty',\n",
       " 'FederatedAI / FATE',\n",
       " 'EssayKillerBrain / EssayKiller_V2',\n",
       " 'JDAI-CV / fast-reid',\n",
       " 'elastic / detection-rules',\n",
       " 'SinaKhalili / Folders.py',\n",
       " 'libratbag / piper',\n",
       " 'vishnubob / wait-for-it']"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_3 = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "html_response_3 = requests.get(url_3).content\n",
    "soup_3 = BeautifulSoup(html_response_3, \"html.parser\") \n",
    "images = [\"https://en.wikipedia.org/\"+re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\", str(element))[0] for element in soup_3.find_all(\"a\", {\"class\": \"image\"})]\n",
    "images"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org//wiki/File:Walt_Disney_1946.JPG',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_Disney_1942_signature.svg',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Trolley_Troubles_poster.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Steamboat-willie.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_Disney_1935.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_Disney_Snow_white_1937_trailer_screenshot_(13).jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Disney_drawing_goofy.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:DisneySchiphol1951.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:WaltDisneyplansDisneylandDec1954.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_disney_portrait_right.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_Disney_Grave.JPG',\n",
       " 'https://en.wikipedia.org//wiki/File:Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Disney_Oscar_1953_(cropped).jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Disney1968.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Disneyland_Resort_logo.svg',\n",
       " 'https://en.wikipedia.org//wiki/File:Animation_disc.svg',\n",
       " 'https://en.wikipedia.org//wiki/File:P_vip.svg',\n",
       " 'https://en.wikipedia.org//wiki/File:Magic_Kingdom_castle.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Video-x-generic.svg',\n",
       " 'https://en.wikipedia.org//wiki/File:Flag_of_Los_Angeles_County,_California.svg',\n",
       " 'https://en.wikipedia.org//wiki/File:Blank_television_set.svg',\n",
       " 'https://en.wikipedia.org//wiki/File:Flag_of_the_United_States.svg']"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_4 ='https://en.wikipedia.org/wiki/Python' "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "source": [
    "soup_4 = BeautifulSoup(requests.get(url_4).content, \"html.parser\") \n",
    "links=[links for links in soup_4.find(\"div\",id=\"bodyContent\").find_all(\"a\")]\n",
    "links_list=[re.findall('href=\"(.*?)\"', str(i))[0] for i in links]\n",
    "links_list=[\"https://en.wikipedia.org\"+link for link in links_list if link[0]==\"/\" ]\n",
    "\n",
    "links_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/Pythonidae',\n",
       " 'https://en.wikipedia.org/wiki/Python_(genus)',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&amp;action=edit&amp;section=1',\n",
       " 'https://en.wikipedia.org/wiki/Python_(programming_language)',\n",
       " 'https://en.wikipedia.org/wiki/CMU_Common_Lisp',\n",
       " 'https://en.wikipedia.org/wiki/PERQ#PERQ_3',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&amp;action=edit&amp;section=2',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Aenus',\n",
       " 'https://en.wikipedia.org/wiki/Python_(painter)',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Byzantium',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Catana',\n",
       " 'https://en.wikipedia.org/wiki/Python_Anghelo',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&amp;action=edit&amp;section=3',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Efteling)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&amp;action=edit&amp;section=4',\n",
       " 'https://en.wikipedia.org/wiki/Python_(automobile_maker)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Ford_prototype)',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&amp;action=edit&amp;section=5',\n",
       " 'https://en.wikipedia.org/wiki/Python_(missile)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(nuclear_primary)',\n",
       " 'https://en.wikipedia.org/wiki/Colt_Python',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&amp;action=edit&amp;section=6',\n",
       " 'https://en.wikipedia.org/wiki/PYTHON',\n",
       " 'https://en.wikipedia.org/wiki/Python_(film)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(mythology)',\n",
       " 'https://en.wikipedia.org/wiki/Monty_Python',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Monty)_Pictures',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&amp;action=edit&amp;section=7',\n",
       " 'https://en.wikipedia.org/wiki/Cython',\n",
       " 'https://en.wikipedia.org/wiki/Pyton',\n",
       " 'https://en.wikipedia.org/wiki/Pithon',\n",
       " 'https://en.wikipedia.org/wiki/File:Disambig_gray.svg',\n",
       " 'https://en.wikipedia.org/wiki/Help:Disambiguation',\n",
       " 'https://en.wikipedia.org/wiki/Help:Category',\n",
       " 'https://en.wikipedia.org/wiki/Category:Disambiguation_pages',\n",
       " 'https://en.wikipedia.org/wiki/Category:Human_name_disambiguation_pages',\n",
       " 'https://en.wikipedia.org/wiki/Category:Disambiguation_pages_with_given-name-holder_lists',\n",
       " 'https://en.wikipedia.org/wiki/Category:Disambiguation_pages_with_short_descriptions',\n",
       " 'https://en.wikipedia.org/wiki/Category:Short_description_is_different_from_Wikidata',\n",
       " 'https://en.wikipedia.org/wiki/Category:All_article_disambiguation_pages',\n",
       " 'https://en.wikipedia.org/wiki/Category:All_disambiguation_pages',\n",
       " 'https://en.wikipedia.org/wiki/Category:Animal_common_name_disambiguation_pages']"
      ]
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Find the number of titles that have changed in the United States Code since its last release point."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_5 = 'http://uscode.house.gov/download/download.shtml'\n",
    "soup_5 = BeautifulSoup(requests.get(url_5).content, \"html.parser\") \n",
    "changed_titles=[changed.text.replace(\"\\n\",\"\").replace(\"٭\",\"\").strip() for changed in soup_5.find_all(\"div\", {\"class\": \"usctitlechanged\"})]\n",
    "changed_titles"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Title 18 - Crimes and Criminal Procedure',\n",
       " 'Title 42 - The Public Health and Welfare']"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Find a Python list with the top ten FBI's Most Wanted names."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_6 = 'https://www.fbi.gov/wanted/topten'\n",
    "soup_6 = BeautifulSoup(requests.get(url_6).content, \"html.parser\") \n",
    "top_wanted=[name.a.text for name in soup_6.find_all(\"h3\", {\"class\": \"title\"})]\n",
    "top_wanted"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'OCTAVIANO JUAREZ-CORRO',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'YULAN ADONAY ARCHAGA CARIAS',\n",
       " 'EUGENE PALMER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'JASON DEREK BROWN']"
      ]
     },
     "metadata": {},
     "execution_count": 152
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "####  Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_7 = 'https://www.emsc-csem.org/Earthquake/'\n",
    "\n",
    "#Extract the body of the table\n",
    "soup_7 = BeautifulSoup(requests.get(url_7).text, \"lxml\") \n",
    "table7=soup_7.find(\"tbody\",id=\"tbody\").find_all(\"td\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "#Extract the titles of the table\n",
    "headers=[]\n",
    "table_headers=soup_7.find(\"tr\",id=\"haut_tableau\")\n",
    "for i in table_headers.find_all(\"th\"):\n",
    "    title=i.text.replace(\"[+]\",\"\").replace(\"[-]\",\"\").strip()\n",
    "    headers.append(title)\n",
    "\n",
    "#Create a dataframe with the headers\n",
    "latests_earthquakes=pd.DataFrame(columns=headers)\n",
    "\n",
    "#Date and Time Column\n",
    "date_time=soup_7.find(\"tbody\",id=\"tbody\").find_all(\"a\")\n",
    "for i in date_time:\n",
    "    if i.find(\"span\"):\n",
    "        i.decompose()\n",
    "latests_earthquakes[\"Date & Time UTC\"]=date_time\n",
    "\n",
    "#Latitude and Longitude Column\n",
    "latitude_longitude=[j.text.replace(\"\\xa0\",\"\").strip() for j in soup_7.find(\"tbody\",id=\"tbody\").find_all(\"td\",{\"class\":\"tabev1\"})]\n",
    "latitude_longitude_letters=[j.text.replace(\"\\xa0\",\"\").strip() for j in soup_7.find(\"tbody\",id=\"tbody\").find_all(\"td\",{\"class\":\"tabev2\"})]\n",
    "\n",
    "for i in latitude_longitude_letters:\n",
    "    if (i!=\"N\" and i!=\"W\" and i!=\"S\" and i!=\"E\"):\n",
    "        latitude_longitude_letters.remove(i)\n",
    "\n",
    "latitude=[]\n",
    "longitude=[]\n",
    "count1=0\n",
    "count2=1\n",
    "while count1 < len(latitude_longitude):\n",
    "    latitude.append(latitude_longitude[count1]+latitude_longitude_letters[count1])\n",
    "    longitude.append(latitude_longitude[count2]+latitude_longitude_letters[count2])\n",
    "    count1+=2\n",
    "    count2+=2\n",
    "\n",
    "latests_earthquakes[\"Latitude degrees\"]=latitude\n",
    "latests_earthquakes[\"Longitude degrees\"]=longitude\n",
    "\n",
    "\n",
    "#Region Name Column\n",
    "region_name=[name.text.replace(\"\\xa0\",\"\").strip() for name in soup_7.find_all(\"td\",{\"class\": \"tb_region\"})]\n",
    "latests_earthquakes[\"Region name\"]=region_name\n",
    "region_name\n",
    "\n",
    "latests_earthquakes=latests_earthquakes.dropna(axis=1)\n",
    "latests_earthquakes[0:29]\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date &amp; Time UTC</th>\n",
       "      <th>Latitude degrees</th>\n",
       "      <th>Longitude degrees</th>\n",
       "      <th>Region name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2021-12-14   14:20:14.0]</td>\n",
       "      <td>7.78S</td>\n",
       "      <td>122.50E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2021-12-14   14:04:05.0]</td>\n",
       "      <td>7.41S</td>\n",
       "      <td>122.40E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2021-12-14   13:59:12.0]</td>\n",
       "      <td>7.74S</td>\n",
       "      <td>122.47E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2021-12-14   13:57:06.0]</td>\n",
       "      <td>7.60S</td>\n",
       "      <td>122.24E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2021-12-14   13:49:26.0]</td>\n",
       "      <td>11.54N</td>\n",
       "      <td>87.56W</td>\n",
       "      <td>NEAR COAST OF NICARAGUA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[2021-12-14   13:48:32.0]</td>\n",
       "      <td>1.85S</td>\n",
       "      <td>120.48E</td>\n",
       "      <td>SULAWESI, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[2021-12-14   13:45:24.0]</td>\n",
       "      <td>7.60S</td>\n",
       "      <td>122.44E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[2021-12-14   13:34:58.0]</td>\n",
       "      <td>7.70S</td>\n",
       "      <td>122.56E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[2021-12-14   13:33:30.0]</td>\n",
       "      <td>7.53S</td>\n",
       "      <td>122.38E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[2021-12-14   13:30:18.0]</td>\n",
       "      <td>7.60S</td>\n",
       "      <td>122.39E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[2021-12-14   13:28:14.2]</td>\n",
       "      <td>17.92N</td>\n",
       "      <td>66.75W</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[2021-12-14   13:23:29.0]</td>\n",
       "      <td>7.82S</td>\n",
       "      <td>122.52E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[2021-12-14   13:10:30.0]</td>\n",
       "      <td>7.70S</td>\n",
       "      <td>122.44E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[2021-12-14   13:09:56.7]</td>\n",
       "      <td>37.04N</td>\n",
       "      <td>1.97W</td>\n",
       "      <td>SPAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[2021-12-14   13:08:17.3]</td>\n",
       "      <td>38.68N</td>\n",
       "      <td>39.47E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[2021-12-14   13:04:32.0]</td>\n",
       "      <td>7.32S</td>\n",
       "      <td>121.25E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[2021-12-14   12:55:06.0]</td>\n",
       "      <td>28.56N</td>\n",
       "      <td>17.83W</td>\n",
       "      <td>CANARY ISLANDS, SPAIN REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[2021-12-14   12:51:06.0]</td>\n",
       "      <td>7.74S</td>\n",
       "      <td>122.45E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[2021-12-14   12:50:04.0]</td>\n",
       "      <td>7.60S</td>\n",
       "      <td>122.30E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[2021-12-14   12:45:35.2]</td>\n",
       "      <td>30.57S</td>\n",
       "      <td>177.33W</td>\n",
       "      <td>KERMADEC ISLANDS, NEW ZEALAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[2021-12-14   12:42:18.0]</td>\n",
       "      <td>29.63S</td>\n",
       "      <td>71.28W</td>\n",
       "      <td>COQUIMBO, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[2021-12-14   12:35:36.0]</td>\n",
       "      <td>7.60S</td>\n",
       "      <td>122.38E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[2021-12-14   12:24:31.0]</td>\n",
       "      <td>7.35S</td>\n",
       "      <td>122.23E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[2021-12-14   12:22:35.0]</td>\n",
       "      <td>0.38N</td>\n",
       "      <td>123.69E</td>\n",
       "      <td>MINAHASA, SULAWESI, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[2021-12-14   12:19:36.5]</td>\n",
       "      <td>28.55N</td>\n",
       "      <td>17.83W</td>\n",
       "      <td>CANARY ISLANDS, SPAIN REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[2021-12-14   12:12:53.1]</td>\n",
       "      <td>60.31N</td>\n",
       "      <td>151.94W</td>\n",
       "      <td>KENAI PENINSULA, ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[2021-12-14   12:12:28.0]</td>\n",
       "      <td>6.60S</td>\n",
       "      <td>121.91E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[2021-12-14   12:05:26.1]</td>\n",
       "      <td>28.56N</td>\n",
       "      <td>17.82W</td>\n",
       "      <td>CANARY ISLANDS, SPAIN REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[2021-12-14   11:52:46.2]</td>\n",
       "      <td>42.71N</td>\n",
       "      <td>15.89E</td>\n",
       "      <td>ADRIATIC SEA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date & Time UTC Latitude degrees Longitude degrees  \\\n",
       "0   [2021-12-14   14:20:14.0]            7.78S           122.50E   \n",
       "1   [2021-12-14   14:04:05.0]            7.41S           122.40E   \n",
       "2   [2021-12-14   13:59:12.0]            7.74S           122.47E   \n",
       "3   [2021-12-14   13:57:06.0]            7.60S           122.24E   \n",
       "4   [2021-12-14   13:49:26.0]           11.54N            87.56W   \n",
       "5   [2021-12-14   13:48:32.0]            1.85S           120.48E   \n",
       "6   [2021-12-14   13:45:24.0]            7.60S           122.44E   \n",
       "7   [2021-12-14   13:34:58.0]            7.70S           122.56E   \n",
       "8   [2021-12-14   13:33:30.0]            7.53S           122.38E   \n",
       "9   [2021-12-14   13:30:18.0]            7.60S           122.39E   \n",
       "10  [2021-12-14   13:28:14.2]           17.92N            66.75W   \n",
       "11  [2021-12-14   13:23:29.0]            7.82S           122.52E   \n",
       "12  [2021-12-14   13:10:30.0]            7.70S           122.44E   \n",
       "13  [2021-12-14   13:09:56.7]           37.04N             1.97W   \n",
       "14  [2021-12-14   13:08:17.3]           38.68N            39.47E   \n",
       "15  [2021-12-14   13:04:32.0]            7.32S           121.25E   \n",
       "16  [2021-12-14   12:55:06.0]           28.56N            17.83W   \n",
       "17  [2021-12-14   12:51:06.0]            7.74S           122.45E   \n",
       "18  [2021-12-14   12:50:04.0]            7.60S           122.30E   \n",
       "19  [2021-12-14   12:45:35.2]           30.57S           177.33W   \n",
       "20  [2021-12-14   12:42:18.0]           29.63S            71.28W   \n",
       "21  [2021-12-14   12:35:36.0]            7.60S           122.38E   \n",
       "22  [2021-12-14   12:24:31.0]            7.35S           122.23E   \n",
       "23  [2021-12-14   12:22:35.0]            0.38N           123.69E   \n",
       "24  [2021-12-14   12:19:36.5]           28.55N            17.83W   \n",
       "25  [2021-12-14   12:12:53.1]           60.31N           151.94W   \n",
       "26  [2021-12-14   12:12:28.0]            6.60S           121.91E   \n",
       "27  [2021-12-14   12:05:26.1]           28.56N            17.82W   \n",
       "28  [2021-12-14   11:52:46.2]           42.71N            15.89E   \n",
       "\n",
       "                      Region name  \n",
       "0                      FLORES SEA  \n",
       "1                      FLORES SEA  \n",
       "2                      FLORES SEA  \n",
       "3                      FLORES SEA  \n",
       "4         NEAR COAST OF NICARAGUA  \n",
       "5             SULAWESI, INDONESIA  \n",
       "6                      FLORES SEA  \n",
       "7                      FLORES SEA  \n",
       "8                      FLORES SEA  \n",
       "9                      FLORES SEA  \n",
       "10             PUERTO RICO REGION  \n",
       "11                     FLORES SEA  \n",
       "12                     FLORES SEA  \n",
       "13                          SPAIN  \n",
       "14                 EASTERN TURKEY  \n",
       "15                     FLORES SEA  \n",
       "16   CANARY ISLANDS, SPAIN REGION  \n",
       "17                     FLORES SEA  \n",
       "18                     FLORES SEA  \n",
       "19  KERMADEC ISLANDS, NEW ZEALAND  \n",
       "20                COQUIMBO, CHILE  \n",
       "21                     FLORES SEA  \n",
       "22                     FLORES SEA  \n",
       "23  MINAHASA, SULAWESI, INDONESIA  \n",
       "24   CANARY ISLANDS, SPAIN REGION  \n",
       "25        KENAI PENINSULA, ALASKA  \n",
       "26                     FLORES SEA  \n",
       "27   CANARY ISLANDS, SPAIN REGION  \n",
       "28                   ADRIATIC SEA  "
      ]
     },
     "metadata": {},
     "execution_count": 145
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Count the number of tweets by a given Twitter account.\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "base_url = 'https://twitter.com/'\n",
    "\n",
    "api_key= \"AXChgfKFHoQEvIUI9wBqtYEqk\"\n",
    "api_key_secret= \"ua574GVbRGnSteG0Dm96GMwnPo76ogymzofZUtRJsN9172Ailz\"\n",
    "access_token= \"232573274-1EO11DKRv3A3sGmEVLuuiliSkZrXyNWoxvG6ITUr\"\n",
    "token_secret = \"4sj8tpZGDLYqxtDeXkhp8o3H9k02d2EqAQPXSoCWnckQT\"\n",
    "\n",
    "# Configuracion de acceso con las credenciales\n",
    "auth = OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(access_token, token_secret)\n",
    " \n",
    "# Listos para hacer la conexion con el API\n",
    "api = tweepy.API(auth)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "source": [
    "user_name= input(str(\"Enter username: \"))\n",
    "\n",
    "try:\n",
    "\n",
    "    # fetching the user\n",
    "    user = api.get_user(screen_name=user_name)\n",
    "    \n",
    "    # fetching the statuses_count attribute\n",
    "    statuses_count = user.statuses_count \n",
    "\n",
    "    print(\"The number of tweets the user \" + user_name + \" has posted are : \" + str(statuses_count))\n",
    "\n",
    "except:\n",
    "     print(\"Username not found\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of tweets the user ironhack has posted are : 4236\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Number of followers of a given twitter account\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'\n",
    "user=input(str(\"Enter username: \"))\n",
    "try:\n",
    "    followers_count = api.get_user(screen_name=user).followers_count\n",
    "    print(\"The number of followers of the user \" + user +\" are : \" + str(followers_count))\n",
    "except:\n",
    "     print(\"Username not found\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of followers of the user ironhack are : 12584\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_10 = 'https://www.wikipedia.org/'\n",
    "soup_10 = BeautifulSoup(requests.get(url_10).content, \"html.parser\") \n",
    "languages=[language.text.strip() for language in soup_10.find_all(\"strong\")][1:-1]\n",
    "articles=[article.text.replace(\"\\xa0\",\"\").strip() for article in soup_10.find_all(\"small\")][0:(len(languages))]\n",
    "\n",
    "languages_articles=pd.DataFrame({\"Language\":languages,\"Articles\":articles})\n",
    "languages_articles"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>6383000+ articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>日本語</td>\n",
       "      <td>1292000+ 記事</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Русский</td>\n",
       "      <td>1756000+ статей</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deutsch</td>\n",
       "      <td>2617000+ Artikel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Español</td>\n",
       "      <td>1717000+ artículos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Français</td>\n",
       "      <td>2362000+ articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>中文</td>\n",
       "      <td>1231000+ 條目</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Italiano</td>\n",
       "      <td>1718000+ voci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Português</td>\n",
       "      <td>1074000+ artigos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Polski</td>\n",
       "      <td>1490000+ haseł</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Language            Articles\n",
       "0    English   6383000+ articles\n",
       "1        日本語         1292000+ 記事\n",
       "2    Русский     1756000+ статей\n",
       "3    Deutsch    2617000+ Artikel\n",
       "4    Español  1717000+ artículos\n",
       "5   Français   2362000+ articles\n",
       "6         中文         1231000+ 條目\n",
       "7   Italiano       1718000+ voci\n",
       "8  Português    1074000+ artigos\n",
       "9     Polski      1490000+ haseł"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_11 = 'https://data.gov.uk/'\n",
    "soup_11 = BeautifulSoup(requests.get(url_11).content, \"html.parser\") \n",
    "datasets=[dataset.text for dataset in soup_11.find_all(\"h3\",{\"class\":\"govuk-heading-s dgu-topics__heading\"})]\n",
    "datasets"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Display the top 10 languages by number of native speakers stored in a pandas dataframe."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_12 = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "soup_12 = BeautifulSoup(requests.get(url_12).text, \"lxml\") \n",
    "top_languages=[language1.text.replace(\"\\n\",\"\").replace(\"[11]\",\"\").replace(\"[12]\",\"\").strip() for language1 in soup_12.find_all(\"td\") if language1.a][0:30:3]\n",
    "speakers=[speaker.text.replace(\"\\n\",\"\").strip() for speaker in soup_12.find_all(\"td\")][4:60:6]\n",
    "top_languages_speakers=pd.DataFrame({\"Language\":top_languages,\"Speakers (millions)\":speakers})\n",
    "top_languages_speakers\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Speakers (millions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi (sanskritised Hindustani)</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Russian</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Western Punjabi</td>\n",
       "      <td>92.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Marathi</td>\n",
       "      <td>83.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Language Speakers (millions)\n",
       "0                 Mandarin Chinese                 918\n",
       "1                          Spanish                 480\n",
       "2                          English                 379\n",
       "3  Hindi (sanskritised Hindustani)                 341\n",
       "4                          Bengali                 300\n",
       "5                       Portuguese                 221\n",
       "6                          Russian                 154\n",
       "7                         Japanese                 128\n",
       "8                  Western Punjabi                92.7\n",
       "9                          Marathi                83.1"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bonus\n",
    "#### Scrape a certain number of tweets of a given Twitter account."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "user=input(str(\"Enter username: \"))\n",
    "n=3 #Number of showed tweets\n",
    "try:\n",
    "     tweets = api.user_timeline(screen_name=user, \n",
    "                              # 200 is the maximum allowed count\n",
    "                              count=200,\n",
    "                              include_rts = False,\n",
    "                              # Necessary to keep full_text \n",
    "                              # otherwise only the first 140 words are extracted\n",
    "                              tweet_mode = 'extended'\n",
    "                              )\n",
    "\n",
    "     for info in tweets[:n]:\n",
    "          print(\"ID: {}\".format(info.id))\n",
    "          print(info.created_at)\n",
    "          print(info.full_text)\n",
    "          print(\"\\n\")\n",
    "except:\n",
    "     print(\"Username not found\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Username not found\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url_13 = 'https://www.imdb.com/chart/top'\n",
    "soup_13 = BeautifulSoup(requests.get(url_13).text, \"lxml\") \n",
    "\n",
    "titles=[title.a.text for title in soup_13.find_all(\"td\",{\"class\":\"titleColumn\"})]\n",
    "\n",
    "release=[release.span.text for release in soup_13.find_all(\"td\",{\"class\":\"titleColumn\"})]\n",
    "\n",
    "directors=[director for director in soup_13.find_all(\"td\",{\"class\":\"titleColumn\"})]\n",
    "directors=[re.findall(\"title=[\\\"\\'](.*?)[\\\"\\']\", str(i))for i in directors]\n",
    "directors=[str(j).split(\"(dir.)\")[0].replace(\"['\",\"\").strip() for j in directors]\n",
    "\n",
    "stars=[star.strong.text for star in soup_13.find_all(\"td\",{\"class\":\"ratingColumn imdbRating\"})]\n",
    "\n",
    "top_movies=pd.DataFrame({\"Movie name\":titles,\"Initial release\":release,\"Director\":directors,\"Stars\":stars})\n",
    "top_movies\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie name</th>\n",
       "      <th>Initial release</th>\n",
       "      <th>Director</th>\n",
       "      <th>Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>(1972)</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>(1974)</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>(2008)</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>(1957)</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Las noches de Cabiria</td>\n",
       "      <td>(1957)</td>\n",
       "      <td>Federico Fellini</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>La princesa prometida</td>\n",
       "      <td>(1987)</td>\n",
       "      <td>Rob Reiner</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Paris, Texas</td>\n",
       "      <td>(1984)</td>\n",
       "      <td>Wim Wenders</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Tres colores: Rojo</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>Krzysztof Kieslowski</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Ratsasan</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>Ram Kumar</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Movie name Initial release              Director Stars\n",
       "0          Cadena perpetua          (1994)        Frank Darabont   9.2\n",
       "1               El padrino          (1972)  Francis Ford Coppola   9.1\n",
       "2     El padrino: Parte II          (1974)  Francis Ford Coppola   9.0\n",
       "3      El caballero oscuro          (2008)     Christopher Nolan   9.0\n",
       "4    12 hombres sin piedad          (1957)          Sidney Lumet   8.9\n",
       "..                     ...             ...                   ...   ...\n",
       "245  Las noches de Cabiria          (1957)      Federico Fellini   8.0\n",
       "246  La princesa prometida          (1987)            Rob Reiner   8.0\n",
       "247           Paris, Texas          (1984)           Wim Wenders   8.0\n",
       "248     Tres colores: Rojo          (1994)  Krzysztof Kieslowski   8.0\n",
       "249               Ratsasan          (2018)             Ram Kumar   8.0\n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "base_url_13='http://www.imdb.com'\n",
    "description_link=[base_url_13+re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\",str(link))[0] for link in soup_13.find_all(\"td\", {\"class\": \"titleColumn\"})]\n",
    "top_movies[\"Link Description\"]=description_link\n",
    "\n",
    "top_10_movies=top_movies[0:10]\n",
    "description=[]\n",
    "for link in top_10_movies[\"Link Description\"]:\n",
    "    soup_i = BeautifulSoup(requests.get(link).content, \"html.parser\") \n",
    "    description_i=[i.span.text for i in soup_i.find_all(\"p\",{\"class\":\"GenresAndPlot__Plot-cum89p-6 bUyrda\"})]\n",
    "    description.append(description_i[0])\n",
    "top_10_movies[\"Description\"]=description\n",
    "top_10=top_10_movies[[\"Movie name\",\"Initial release\",\"Description\"]]\n",
    "top_10"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/ivan.repilado/miniconda3/envs/ironhack_env/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie name</th>\n",
       "      <th>Initial release</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>Two imprisoned men bond over a number of years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>(1972)</td>\n",
       "      <td>The Godfather follows Vito Corleone, Don of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>(1974)</td>\n",
       "      <td>The early life and career of Vito Corleone in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>(2008)</td>\n",
       "      <td>When the menace known as the Joker wreaks havo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>(1957)</td>\n",
       "      <td>The jury in a New York City murder trial is fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>La lista de Schindler</td>\n",
       "      <td>(1993)</td>\n",
       "      <td>In German-occupied Poland during World War II,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>El señor de los anillos: El retorno del rey</td>\n",
       "      <td>(2003)</td>\n",
       "      <td>Gandalf and Aragorn lead the World of Men agai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pulp Fiction</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>The lives of two mob hitmen, a boxer, a gangst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>El bueno, el feo y el malo</td>\n",
       "      <td>(1966)</td>\n",
       "      <td>A bounty hunting scam joins two men in an unea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>El señor de los anillos: La comunidad del anillo</td>\n",
       "      <td>(2001)</td>\n",
       "      <td>A meek Hobbit from the Shire and eight compani...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Movie name Initial release  \\\n",
       "0                                   Cadena perpetua          (1994)   \n",
       "1                                        El padrino          (1972)   \n",
       "2                              El padrino: Parte II          (1974)   \n",
       "3                               El caballero oscuro          (2008)   \n",
       "4                             12 hombres sin piedad          (1957)   \n",
       "5                             La lista de Schindler          (1993)   \n",
       "6       El señor de los anillos: El retorno del rey          (2003)   \n",
       "7                                      Pulp Fiction          (1994)   \n",
       "8                        El bueno, el feo y el malo          (1966)   \n",
       "9  El señor de los anillos: La comunidad del anillo          (2001)   \n",
       "\n",
       "                                         Description  \n",
       "0  Two imprisoned men bond over a number of years...  \n",
       "1  The Godfather follows Vito Corleone, Don of th...  \n",
       "2  The early life and career of Vito Corleone in ...  \n",
       "3  When the menace known as the Joker wreaks havo...  \n",
       "4  The jury in a New York City murder trial is fr...  \n",
       "5  In German-occupied Poland during World War II,...  \n",
       "6  Gandalf and Aragorn lead the World of Men agai...  \n",
       "7  The lives of two mob hitmen, a boxer, a gangst...  \n",
       "8  A bounty hunting scam joins two men in an unea...  \n",
       "9  A meek Hobbit from the Shire and eight compani...  "
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "source": [
    "#https://openweathermap.org/current\n",
    "city = input('Enter the city: ')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'\n",
    "\n",
    "#La key que nos dais no es válida."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "{\"cod\":401, \"message\": \"Invalid API key. Please see http://openweathermap.org/faq#error401 for more info.\"}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url_16 = 'http://books.toscrape.com/'\n",
    "soup_16 = BeautifulSoup(requests.get(url_16).content, \"html.parser\") \n",
    "book_name=[book for book in soup_16.find_all(\"article\",{\"class\":\"product_pod\"})]\n",
    "book_name=[re.findall('title=\"(.*?)\"', str(i))[0] for i in book_name]\n",
    "price=[price.text for price in soup_16.find_all(\"p\",{\"class\":\"price_color\"})]\n",
    "stock=[stock.text.replace(\"\\n\",\"\").strip() for stock in soup_16.find_all(\"p\",{\"class\":\"instock availability\"})]\n",
    "products=pd.DataFrame({\"Book name\":book_name,\"Price\":price,\"Availability\":stock})\n",
    "products\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Availability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>£22.65</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dirty Little Secrets of Getting Your Dream...</td>\n",
       "      <td>£33.34</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Coming Woman: A Novel Based on the Life of...</td>\n",
       "      <td>£17.93</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boys in the Boat: Nine Americans and Their...</td>\n",
       "      <td>£22.60</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>£52.15</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Starving Hearts (Triangular Trade Trilogy, #1)</td>\n",
       "      <td>£13.99</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>£20.66</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>£17.46</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scott Pilgrim's Precious Little Life (Scott Pi...</td>\n",
       "      <td>£52.29</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rip it Up and Start Again</td>\n",
       "      <td>£35.02</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Our Band Could Be Your Life: Scenes from the A...</td>\n",
       "      <td>£57.25</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Olio</td>\n",
       "      <td>£23.88</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mesaerion: The Best Science Fiction Stories 18...</td>\n",
       "      <td>£37.59</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>£51.33</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>£45.17</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Book name   Price Availability\n",
       "0                                A Light in the Attic  £51.77     In stock\n",
       "1                                  Tipping the Velvet  £53.74     In stock\n",
       "2                                          Soumission  £50.10     In stock\n",
       "3                                       Sharp Objects  £47.82     In stock\n",
       "4               Sapiens: A Brief History of Humankind  £54.23     In stock\n",
       "5                                     The Requiem Red  £22.65     In stock\n",
       "6   The Dirty Little Secrets of Getting Your Dream...  £33.34     In stock\n",
       "7   The Coming Woman: A Novel Based on the Life of...  £17.93     In stock\n",
       "8   The Boys in the Boat: Nine Americans and Their...  £22.60     In stock\n",
       "9                                     The Black Maria  £52.15     In stock\n",
       "10     Starving Hearts (Triangular Trade Trilogy, #1)  £13.99     In stock\n",
       "11                              Shakespeare's Sonnets  £20.66     In stock\n",
       "12                                        Set Me Free  £17.46     In stock\n",
       "13  Scott Pilgrim's Precious Little Life (Scott Pi...  £52.29     In stock\n",
       "14                          Rip it Up and Start Again  £35.02     In stock\n",
       "15  Our Band Could Be Your Life: Scenes from the A...  £57.25     In stock\n",
       "16                                               Olio  £23.88     In stock\n",
       "17  Mesaerion: The Best Science Fiction Stories 18...  £37.59     In stock\n",
       "18                       Libertarianism for Beginners  £51.33     In stock\n",
       "19                            It's Only the Himalayas  £45.17     In stock"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('ironhack_env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "interpreter": {
   "hash": "8c5c2b57ca15aef8b3df4b1c6669ccf99964c5a816b33fa10768565149e83d50"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}